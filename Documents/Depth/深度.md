# 深度

深度是计算机图形学中的一个术语，它指的是一个片元（潜在的像素）离摄像机有多远。但这可能有点复杂，因为深度可能来自不同的空间/范围，不同的平台，以及不同的透视和正交摄像机投影。

![Image](./Images/00.jpg)

这篇文章回顾了我所遇到的关于深度的一切（所以你可以说这是一篇有深度的关于深度的文章!），主要专注于 Unity 以及 Universal RP （但也包含一些 HDRP 的内容 ）。包括 ShaderGraph 和着色器代码（HLSL）的信息。

## 什么是深度？

### 视图空间，眼睛深度

最容易理解的深度类型是眼睛/视图空间深度，指的是场景中的某个位置到垂直于摄像机视图的平面之间的距离 ——（不是到摄像机本身的位置）。

![Image](./Images/01.jpg)

在顶点着色阶段，一个网格中对象空间的顶点位置通过模型矩阵变换到世界空间，然后通过视图矩阵变换到视图空间。在视图空间中，位置是相对于摄像机而言的，而摄像机看向 Z 轴的负方向。

通过对视图空间位置的 Z 值求反，将其变回正值（假设对象位于摄像机前面），也就是眼睛深度。深度值 0 在摄像机的位置，1 表示距离为 1 个单位，10 表示距离为 10 个单位，以此类推。我也听到过这个深度值被称为“世界”空间深度，可能由于世界空间和视图空间都使用了同样的单位比例，但是就个人来说，我会坚持使用视图或眼睛来命名。

所有平台的深度范围相同。它同样也适用于正交和透视投影，因为视图空间在应用投影之前。你确实需要注意如何获得眼睛深度！

对于着色器代码来说：

```hlsl
// Vertex : (Should work in URP and HDRP)
float3 positionWS = TransformObjectToWorld(IN.positionOS.xyz);
float3 positionVS = TransformWorldToView(positionWS);
OUT.positionVS = positionVS;

// Fragment :
float fragmentEyeDepth = -IN.positionVS.z;

// Also equivalent to : (even less pipeline specific)
// Vertex :
float3 positionWS = mul(UNITY_MATRIX_M, float4(IN.positionOS.xyz, 1.0)).xyz;
float3 positionVS = mul(UNITY_MATRIX_V, float4(positionWS, 1.0)).xyz;
OUT.positionVS = positionVS;

// Fragment :
float fragmentEyeDepth = -IN.positionVS.z;

// 所有这些都需要 positionVS 包含在使用 TEXCOORDX 语义之一传递到片段着色器的结构中，假设你不只是想要顶点的眼睛深度。
// 我猜你也可以只传递 z/depth 而不是整个视图空间位置。
```

为了获得场景中对象的眼睛深度，我们可以对 Unity 为我们生成的一种特殊纹理（**Depth Texture**）进行采样。

你通常还会看到使用 **Linear01** 深度，这只是眼睛深度的重映射版本（通过除以远平面值）。摄像机位置仍为 0，但远平面位置为 1。

## 裁剪空间，标准化设备坐标（NDC）& Z 缓冲深度

视图空间位置通过**投影矩阵**变换到**裁剪空间**。通常像这样的变换不能以非均匀的方式扭曲空间，这是透视投影需要的，但是矩阵的设置方式是在结果的 **W** 分量中输出**眼睛深度**。然后顶点着色器输出该裁剪空间位置。对正交投影来说，W 分量只会以 1 结尾。

在顶点和片元阶段之间，裁剪空间 XYZ 被重新映射，然后被其 W 分量分割（称为**透视分割**）。发生的重映射与在 VertexPositionInputs 结构中计算的 ComputeScreenPos 函数或 positionNDC 相同。在 URP 中，它们都可以在 [<u>ShaderVariablesFunctions.hlsl</u>](https://github.com/Unity-Technologies/Graphics/blob/master/com.unity.render-pipelines.universal/ShaderLibrary/ShaderVariablesFunctions.hlsl) 中找到。

```hlsl
// Clip Space Position (calculated for vertex shader SV_POSITION output)
float4 positionCS = TransformWorldToHClip(input.positionOS.xyz);
OUT.positionCS = positionCS;

// 重映射，自动处理 SV_POSITION 语义。
// 请注意，它的 w 分量（眼睛深度）保持不变，即使被传递到片段中。
// 其它语义（TEXCOORDX）传递裁剪空间需要手动完成。
float4 positionNDC = positionCS * 0.5f;
positionNDC.xy = float2(positionNDC.x, positionNDC.y*_ProjectionParams.x) + positionNDC.w;
positionNDC.zw = positionCS.zw;
OUT.positionNDC = positionNDC;

// 或者直接
// OUT.positionNDC = ComputeScreenPos(positionCS);

// 透视分割（在片元着色器中处理）
float3 pd = IN.positionNDC.xyz / IN.positionNDC.w;
float2 screenUV = pd.xy;
float depth = pd.z;// for OpenGL, also need * 0.5 + 0.5;
```

这为我们提供了标准设备坐标（NDC），也就是屏幕位置，它的 XY 轴的范围从左下角的（0，0）到右上角的（1，1）—— 至少在 Unity/URP 中是这样的。这与 Shader Graph 中 Screen Position 节点是一样的，使用它的默认模式 ... 但尴尬的是，使用该模式时 Z 轴好像不会被传递。Z 轴可以通过 Raw 模式获取，但这是裁剪空间 Z/Depth 。然后我们需要通过 Z 除以 W 的处理来取得 NDC Z/Depth 。

请注意，这个深度在视图空间不再是线性的，这是由于投影矩阵带来的变化，至少在透视投影中是这样。这是在深度缓冲中保留的值，也是在深度纹理或 Raw Screen Depth 节点中保留的值。

这个 NDC.z / Z Buffer Depth 的范围对不同的投影都是一样的，但因平台而异：

- 类似 Direct3D，反向 Z Buffer：近平面为 1，远平面为 0。
- 类似 OpenGL，Z Buffer：近平面为 0，远平面为 1。

如果感兴趣，裁剪空间 Z 值的范围也如下所示，但我不确定是否真的有任何情况下，你会使用它超过其他深度值。

透视投影

- 类似 Direct3D：近平面值在近平面，0 在远平面
- 类似 OpenGL：负的近平面距离在近平面，远平面距离在远平面

正交投影

- 类似 Direct3D 的： 
- 类似 OpenGL 的：

## 为什么是非线性？

将深度缓冲区值设置为非线性（在视图空间中）以及对类似 Direct3D 的平台反转 Z 缓冲区的原因，是为了获得更好的精度。在这篇 NVIDIA 的文章中可以找到更多关于这一点的信息，它可以比我更好地解释这一点。

这里有张图片有助于显示非线性 NDC / Z 缓冲区深度（此处标记为“raw”，因为它是深度缓冲区和深度纹理的原始值）与 Linear01 或眼睛深度之间的比较。

![image](./Images/03.jpg)

右边是 Linear01 数值乘以 15 （远平面值）- 等于眼睛深度，通过一个 frac 函数输入，使值在 0 和 1 之间重复。在右侧每个在 0-1 之间重复的值是 1 个单位。当接近远平面时，每个重复部分的宽度变小。

在左侧，我们将 NDC 深度乘以 15（远平面）并再次输入一个 frac 函数。然而此深度在视图空间是非线性的，它最终在 NDC/屏幕空间是线性的 - 每个重复部分的宽度根据观察角度而有所不同，但在整个平面上总是一样的。我看到这篇文章再次确认了这一点，并指出它对一些硬件优化及后期处理非常有用，例如比较相邻像素的深度值进行边缘监测。
